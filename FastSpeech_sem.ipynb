{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhJ98OWOcWFw",
    "outputId": "fe02b780-ed90-49cc-d421-854e7527bb57",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash \n",
    "# #install libraries\n",
    "# pip install torchaudio\n",
    "# pip install wandb\n",
    "# pip install gdown==4.5.4 --no-cache-dir\n",
    "# pip install unidecode\n",
    "# pip install pyworld\n",
    "\n",
    "# #download LjSpeech\n",
    "# wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 -o /dev/null\n",
    "# mkdir data\n",
    "# tar -xvf LJSpeech-1.1.tar.bz2 >> /dev/null\n",
    "# mv LJSpeech-1.1 data/LJSpeech-1.1\n",
    "\n",
    "# gdown https://drive.google.com/u/0/uc?id=1-EdH0t0loc6vPiuVtXdhsDtzygWNSNZx\n",
    "# mv train.txt data/\n",
    "\n",
    "# #download Waveglow\n",
    "# gdown https://drive.google.com/u/0/uc?id=1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx\n",
    "# mkdir -p waveglow/pretrained_model/\n",
    "# mv waveglow_256channels_ljs_v2.pt waveglow/pretrained_model/waveglow_256channels.pt\n",
    "\n",
    "# gdown https://drive.google.com/u/0/uc?id=1cJKJTmYd905a-9GFoo5gKjzhKjUVj83j\n",
    "# tar -xvf mel.tar.gz\n",
    "# echo $(ls mels | wc -l)\n",
    "\n",
    "# #download alignments\n",
    "# wget https://github.com/xcmyz/FastSpeech/raw/master/alignments.zip\n",
    "# unzip alignments.zip >> /dev/null\n",
    "\n",
    "# # we will use waveglow code, data and audio preprocessing from this repo\n",
    "# git clone https://github.com/xcmyz/FastSpeech.git\n",
    "# mv FastSpeech/text .\n",
    "# mv FastSpeech/audio .\n",
    "# mv FastSpeech/waveglow/* waveglow/\n",
    "# mv FastSpeech/utils.py .\n",
    "# mv FastSpeech/glow.py .\n",
    "\n",
    "## buffer download\n",
    "# wget https://downloader.disk.yandex.ru/disk/c8ea74d69a869a35c813646211c683db3429fcf892fb189858a609a81dda34ed/6383cc33/4Cq0mTlOZVquoBI7sTsS-3hn1BlJ_r8qWORxX7vkyYze9v_T0OFdEu9592I5H6HlqqVbE57XMKLmo3_qwpOFJg%3D%3D?uid=0&filename=saved_buffer.pkl&disposition=attachment&hash=Ogd5xNmUIYGryy5hvm1RJqqqU77x56BifoVPs%2BSpfT%2BKcd%2BfNCwU2jvIHDo%2BdhbMq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=783862266&fsize=2508360771&hid=a36b2d6dc2aae81ccadf21807deecc8e&media_type=compressed&tknv=v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1XEo6WrJcXlE"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    num_mels = 80\n",
    "    max_wav_value = 32768.0\n",
    "    sampling_rate = 22050\n",
    "    filter_length = 1024\n",
    "    hop_length = 256\n",
    "    win_length = 1024\n",
    "    mel_fmin = 0.0\n",
    "    mel_fmax = 8000.0\n",
    "\n",
    "@dataclass\n",
    "class FastSpeechConfig:\n",
    "    vocab_size = 300\n",
    "    max_seq_len = 3000\n",
    "\n",
    "    encoder_dim = 256\n",
    "    encoder_n_layer = 4\n",
    "    encoder_head = 2\n",
    "    encoder_conv1d_filter_size = 1024\n",
    "\n",
    "    decoder_dim = 256\n",
    "    decoder_n_layer = 4\n",
    "    decoder_head = 2\n",
    "    decoder_conv1d_filter_size = 1024\n",
    "\n",
    "    n_bins = 256\n",
    "    min_energy = 0\n",
    "    max_energy = 488\n",
    "    min_pitch = 1\n",
    "    max_pitch = 900\n",
    "\n",
    "    fft_conv1d_kernel = (9, 1)\n",
    "    fft_conv1d_padding = (4, 0)\n",
    "\n",
    "    duration_predictor_filter_size = 256\n",
    "    duration_predictor_kernel_size = 3\n",
    "    dropout = 0.1\n",
    "    \n",
    "    PAD = 0\n",
    "    UNK = 1\n",
    "    BOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<unk>'\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    checkpoint_path = \"./model_new\"\n",
    "    logger_path = \"./logger\"\n",
    "    mel_ground_truth = \"./mels\"\n",
    "    alignment_path = \"./alignments\"\n",
    "    data_path = './data/train.txt'\n",
    "    wav_path = './data/LJSpeech-1.1/wavs'\n",
    "    \n",
    "    wandb_project = 'fastspeech2'\n",
    "    \n",
    "    text_cleaners = ['english_cleaners']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    batch_size = 16\n",
    "    epochs = 2000\n",
    "    n_warm_up_step = 4000\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-6\n",
    "    grad_clip_thresh = 1.0\n",
    "    decay_step = [500000, 1000000, 2000000]\n",
    "\n",
    "    save_step = 3000\n",
    "    log_step = 5\n",
    "    clear_Time = 20\n",
    "\n",
    "    batch_expand_size = 32\n",
    "    \n",
    "\n",
    "mel_config = MelSpectrogramConfig()\n",
    "model_config = FastSpeechConfig()\n",
    "train_config = TrainConfig()\n",
    "stft = lambda wav: torch.stft(\n",
    "    wav,\n",
    "    mel_config.filter_length,\n",
    "    mel_config.hop_length,\n",
    "    mel_config.win_length,\n",
    "    return_complex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import text_to_sequence\n",
    "import pyworld as pw\n",
    "\n",
    "\n",
    "def pad_1D(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = np.pad(x, (0, length - x.shape[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_1D_tensor(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = F.pad(x, (0, length - x.shape[0]))\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = torch.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_2D(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        PAD = 0\n",
    "        if np.shape(x)[0] > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = np.shape(x)[1]\n",
    "        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = np.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(np.shape(x)[0] for x in inputs)\n",
    "        output = np.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def pad_2D_tensor(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        if x.size(0) > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = x.size(1)\n",
    "        x_padded = F.pad(x, (0, 0, 0, max_len-x.size(0)))\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = torch.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(x.size(0) for x in inputs)\n",
    "        output = torch.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_text(train_text_path):\n",
    "    with open(train_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = []\n",
    "        for line in f.readlines():\n",
    "            txt.append(line)\n",
    "\n",
    "        return txt\n",
    "\n",
    "\n",
    "def get_data_to_buffer(train_config):\n",
    "    buffer = list()\n",
    "    text = process_text(train_config.data_path)\n",
    "    wavs = os.listdir(train_config.wav_path)\n",
    "    wavs.sort()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for i in tqdm(range(len(text))):\n",
    "\n",
    "        mel_gt_name = os.path.join(\n",
    "            train_config.mel_ground_truth, \"ljspeech-mel-%05d.npy\" % (i+1))\n",
    "        wav = torchaudio.load(os.path.join(train_config.wav_path, wavs[i]))[0].squeeze(0)\n",
    "        \n",
    "        mel_gt_target = np.load(mel_gt_name)\n",
    "        duration = np.load(os.path.join(\n",
    "            train_config.alignment_path, str(i)+\".npy\"))\n",
    "        character = text[i][0:len(text[i])-1]\n",
    "        character = np.array(\n",
    "            text_to_sequence(character, train_config.text_cleaners))\n",
    "\n",
    "        character = torch.from_numpy(character)\n",
    "        duration = torch.from_numpy(duration)\n",
    "        mel_gt_target = torch.from_numpy(mel_gt_target)\n",
    "        \n",
    "        pitch, t = pw.dio(\n",
    "            wav.numpy().astype(np.float64),\n",
    "            mel_config.sampling_rate,\n",
    "            frame_period=mel_config.hop_length / mel_config.sampling_rate * 1000,\n",
    "        )\n",
    "        pitch = pw.stonemask(wav.numpy().astype(np.float64), pitch, t, mel_config.sampling_rate)\n",
    "        energy = stft(wav).square().sum((-1, 0)).sqrt()\n",
    "\n",
    "        buffer.append({\n",
    "            \"text\": character, \"duration\": duration,\n",
    "            \"mel_target\": mel_gt_target, \"pitch\": pitch,\n",
    "            \"energy\": energy, \"speaker_id\": int(wavs[i].split('-')[0][-2:])\n",
    "        })\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"cost {:.2f}s to load all data into buffer.\".format(end-start))\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, buffer):\n",
    "        self.buffer = buffer\n",
    "        self.length_dataset = len(self.buffer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "\n",
    "\n",
    "def reprocess_tensor(batch, cut_list):\n",
    "    texts = [batch[ind][\"text\"] for ind in cut_list]\n",
    "    mel_targets = [batch[ind][\"mel_target\"] for ind in cut_list]\n",
    "    energy_targets = [batch[ind][\"energy\"] for ind in cut_list]\n",
    "    pitch_targets = [batch[ind][\"pitch\"] for ind in cut_list]\n",
    "    speakers = [batch[ind][\"speaker_id\"] for ind in cut_list]\n",
    "    durations = [batch[ind][\"duration\"] for ind in cut_list]\n",
    "\n",
    "    length_text = np.array([])\n",
    "    for text in texts:\n",
    "        length_text = np.append(length_text, text.size(0))\n",
    "\n",
    "    src_pos = list()\n",
    "    max_len = int(max(length_text))\n",
    "    for length_src_row in length_text:\n",
    "        src_pos.append(np.pad([i+1 for i in range(int(length_src_row))],\n",
    "                              (0, max_len-int(length_src_row)), 'constant'))\n",
    "    src_pos = torch.from_numpy(np.array(src_pos))\n",
    "\n",
    "    length_mel = np.array(list())\n",
    "    for mel in mel_targets:\n",
    "        length_mel = np.append(length_mel, mel.size(0))\n",
    "\n",
    "    mel_pos = list()\n",
    "    max_mel_len = int(max(length_mel))\n",
    "    for length_mel_row in length_mel:\n",
    "        mel_pos.append(np.pad([i+1 for i in range(int(length_mel_row))],\n",
    "                              (0, max_mel_len-int(length_mel_row)), 'constant'))\n",
    "    mel_pos = torch.from_numpy(np.array(mel_pos))\n",
    "\n",
    "    texts = pad_1D_tensor(texts)\n",
    "    durations = pad_1D_tensor(durations)\n",
    "    energy_targets = pad_1D_tensor(energy_targets)\n",
    "    pitch_targets = pad_1D_tensor([torch.from_numpy(pitch) for pitch in pitch_targets])\n",
    "    mel_targets = pad_2D_tensor(mel_targets)\n",
    "\n",
    "    out = {\"text\": texts,\n",
    "           \"mel_target\": mel_targets,\n",
    "           \"energy_target\": energy_targets,\n",
    "           \"pitch_target\": pitch_targets,\n",
    "           \"speaker_id\": speakers,\n",
    "           \"duration\": durations,\n",
    "           \"mel_pos\": mel_pos,\n",
    "           \"src_pos\": src_pos,\n",
    "           \"mel_max_len\": max_mel_len}\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_fn_tensor(batch):\n",
    "    len_arr = np.array([d[\"text\"].size(0) for d in batch])\n",
    "    index_arr = np.argsort(-len_arr)\n",
    "    batchsize = len(batch)\n",
    "    real_batchsize = batchsize // train_config.batch_expand_size\n",
    "\n",
    "    cut_list = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n",
    "\n",
    "    output = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        output.append(reprocess_tensor(batch, cut_list[i]))\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "del buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13100/13100 [23:04<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 1384.93s to load all data into buffer.\n"
     ]
    }
   ],
   "source": [
    "# buffer = get_data_to_buffer(train_config)\n",
    "# torch.save(buffer, 'saved_buffer.pkl')\n",
    "\n",
    "buffer = torch.load('saved_buffer.pkl')\n",
    "dataset = BufferDataset(buffer)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_config.batch_expand_size * train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_tensor,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have computer q,k,v matricies and we should calc attention score and calc weighted value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20190325121034288.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -torch.inf)\n",
    "        attn = self.softmax(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_attention = ScaledDotProductAttention(1)\n",
    "\n",
    "q = torch.randn(4 * 4, 8, 4)\n",
    "k = torch.randn(4 * 4, 8, 4)\n",
    "v = torch.randn(4 * 4, 8, 4)\n",
    "\n",
    "output, attn = dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на аттэншн мапы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(24, 4))\n",
    "for i, at in enumerate(attn[0:4]):\n",
    "    sns.heatmap(attn[i], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/attention_picture.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут инпуты тремя линейными сломи преобразуем для подачи в selt attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=d_k**0.5) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # normal distribution initialization better than kaiming(default in pytorch)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_v))) \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realization from Andrey Karpathy- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "- Flash Attention - https://arxiv.org/abs/2205.14135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двухслойная сеть из 1d конволюций с релу в качестве активации. После идет драпаут и layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use Conv1D\n",
    "        # position-wise\n",
    "        self.w_1 = nn.Conv1d(\n",
    "            d_in, d_hid, kernel_size=model_config.fft_conv1d_kernel[0], padding=model_config.fft_conv1d_padding[0])\n",
    "        # position-wise\n",
    "        self.w_2 = nn.Conv1d(\n",
    "            d_hid, d_in, kernel_size=model_config.fft_conv1d_kernel[1], padding=model_config.fft_conv1d_padding[1])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFTBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/fft.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещаем все вместе в один FFT(Feed Forward Transformer) BLock. Теперь можно стакать эти слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(torch.nn.Module):\n",
    "    \"\"\"FFT Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 d_inner,\n",
    "                 n_head,\n",
    "                 d_k,\n",
    "                 d_v,\n",
    "                 dropout=0.1):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "intermediate_size = 64\n",
    "n_head = 4\n",
    "batch_size = 4\n",
    "seq_len = 12\n",
    "\n",
    "fft_block = FFTBlock(hidden_size, intermediate_size, n_head, hidden_size // n_head, hidden_size // n_head)\n",
    "\n",
    "inp_tensor = torch.rand(batch_size, seq_len, hidden_size, dtype=torch.float32)\n",
    "\n",
    "out_tensor = fft_block(inp_tensor)[0]\n",
    "\n",
    "assert inp_tensor.shape == out_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training tips - https://arxiv.org/pdf/1804.00247.pdf\n",
    "- Transformer without Tears - https://tnq177.github.io/data/transformers_without_tears.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Regulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignment(base_mat, duration_predictor_output):\n",
    "    N, L = duration_predictor_output.shape\n",
    "    for i in range(N):\n",
    "        count = 0\n",
    "        for j in range(L):\n",
    "            for k in range(duration_predictor_output[i][j]):\n",
    "                base_mat[i][count+k][j] = 1\n",
    "            count = count + duration_predictor_output[i][j]\n",
    "    return base_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция которая делает бинарную матрицу для деблирование каждого мела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_alignment(\n",
    "    torch.zeros(1, 6, 3).numpy(),\n",
    "    torch.LongTensor([[1,2,3]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/duration_predictor.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super().__init__()\n",
    "        self.dim_1 = dim_1\n",
    "        self.dim_2 = dim_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim_1, self.dim_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простая сетка с двумя конволюшналами и Линейным слоем агрегатом. Предсказываем тут длительность каждой фонемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\" Duration Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(DurationPredictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config.encoder_dim\n",
    "        self.filter_size = model_config.duration_predictor_filter_size\n",
    "        self.kernel = model_config.duration_predictor_kernel_size\n",
    "        self.conv_output_size = model_config.duration_predictor_filter_size\n",
    "        self.dropout = model_config.dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        encoder_output = self.conv_net(encoder_output)\n",
    "            \n",
    "        out = self.linear_layer(encoder_output)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if not self.training:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_predictor = DurationPredictor(model_config)\n",
    "\n",
    "inp_tensor = torch.rand(\n",
    "    2, # batch_size\n",
    "    12, #seq_len\n",
    "    model_config.encoder_dim,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "dur_prediction = dur_predictor(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4558, 0.2430, 0.0000, 0.0000, 0.1493, 0.0875, 0.0473, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.3733],\n",
       "        [0.4637, 0.0000, 0.4236, 0.0209, 0.0000, 0.0000, 0.0136, 0.1392, 0.0908,\n",
       "         0.1492, 0.0000, 0.0000]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\" Length Regulator \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "        self.duration_predictor = DurationPredictor(model_config)\n",
    "\n",
    "    def LR(self, x, duration_predictor_output, mel_max_length=None):\n",
    "        expand_max_len = torch.max(\n",
    "            torch.sum(duration_predictor_output, -1), -1)[0]\n",
    "        alignment = torch.zeros(duration_predictor_output.size(0),\n",
    "                                expand_max_len,\n",
    "                                duration_predictor_output.size(1)).numpy()\n",
    "        alignment = create_alignment(alignment,\n",
    "                                     duration_predictor_output.cpu().numpy())\n",
    "        alignment = torch.from_numpy(alignment).to(x.device)\n",
    "\n",
    "        output = alignment @ x\n",
    "        if mel_max_length:\n",
    "            output = F.pad(\n",
    "                output, (0, 0, 0, mel_max_length-output.size(1), 0, 0))\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, alpha=1.0, target=None, mel_max_length=None):\n",
    "        duration_predictor_output = self.duration_predictor(x)\n",
    "\n",
    "        if target is not None:\n",
    "            output = self.LR(x, target, mel_max_length=mel_max_length)\n",
    "            return output, duration_predictor_output\n",
    "        else:\n",
    "            duration_predictor_output = (\n",
    "                (duration_predictor_output + 0.5) * alpha).int()\n",
    "            output = self.LR(x, duration_predictor_output)\n",
    "            mel_pos = torch.stack(\n",
    "                [torch.Tensor([i+1 for i in range(output.size(1))])]).long()\n",
    "\n",
    "            return output, mel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceAdaptor(nn.Module):\n",
    "    \"\"\" Duration Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(VarianceAdaptor, self).__init__()\n",
    "\n",
    "        self.length_regulator = LengthRegulator(model_config)\n",
    "        self.pitch_predictor = DurationPredictor(model_config)\n",
    "        self.energy_predictor = DurationPredictor(model_config)\n",
    "\n",
    "        self.energy_embeds = nn.Embedding(model_config.n_bins, model_config.encoder_dim)\n",
    "        self.pitch_embeds = nn.Embedding(model_config.n_bins, model_config.encoder_dim)\n",
    "        self.energy_bins = nn.Parameter(\n",
    "            torch.linspace(model_config.min_energy, model_config.max_energy, model_config.n_bins - 1),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        self.pitch_bins = nn.Parameter(\n",
    "            torch.logspace(np.log(model_config.min_pitch), np.log(model_config.max_pitch), model_config.n_bins - 1),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "\n",
    "    def embed_pitch(self, x, target):\n",
    "        pred = self.pitch_predictor(x)\n",
    "        if target is not None:\n",
    "            embedding = self.pitch_embeds(torch.bucketize(target, self.pitch_bins))\n",
    "        else:\n",
    "            embedding = self.pitch_embeds(torch.bucketize(pred, self.pitch_bins))\n",
    "        return pred, embedding\n",
    "\n",
    "    def embed_energy(self, x, target):\n",
    "        pred = self.energy_predictor(x)\n",
    "        if target is not None:\n",
    "            embedding = self.energy_embeds(torch.bucketize(target, self.energy_bins))\n",
    "        else:\n",
    "            embedding = self.energy_embeds(torch.bucketize(pred, self.energy_bins))\n",
    "        return pred, embedding\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_output,\n",
    "        mel_max_length=None,\n",
    "        length_target=None,\n",
    "        pitch_target=None,\n",
    "        energy_target=None,\n",
    "        alpha=1.0\n",
    "    ):\n",
    "        duration_prediction = None\n",
    "        mel_pos = None\n",
    "        if self.training:\n",
    "            length_regulator_output, duration_prediction = self.length_regulator(encoder_output,\n",
    "                                                                                       target=length_target,\n",
    "                                                                                       alpha=alpha,\n",
    "                                                                                       mel_max_length=mel_max_length)\n",
    "        else:\n",
    "            length_regulator_output, mel_pos = self.length_regulator(encoder_output, alpha=alpha)\n",
    "        \n",
    "        energy_prediction, energy_embedding = self.embed_energy(\n",
    "            length_regulator_output, energy_target\n",
    "        )\n",
    "        pitch_prediction, pitch_embedding = self.embed_pitch(\n",
    "            length_regulator_output, pitch_target\n",
    "        )\n",
    "        length_regulator_output = length_regulator_output + pitch_embedding + energy_embedding\n",
    "\n",
    "        return (\n",
    "            length_regulator_output,\n",
    "            mel_pos,\n",
    "            duration_prediction,\n",
    "            energy_prediction,\n",
    "            pitch_prediction,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс который все объеденяет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final BLock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_pad_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(model_config.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(model_config.PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(\n",
    "        1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер и Декодер, берем FFT слои и цикликом по ним проходим. В энкодере эмбединги токенов и позишн эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.encoder_n_layer\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(\n",
    "            model_config.vocab_size,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)\n",
    "        \n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list += [enc_slf_attn]\n",
    "        \n",
    "\n",
    "        return enc_output, non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.decoder_n_layer\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD,\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_seq, enc_pos, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=enc_pos, seq_q=enc_pos)\n",
    "        non_pad_mask = get_non_pad_mask(enc_pos)\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = enc_seq + self.position_enc(enc_pos)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn = dec_layer(\n",
    "                dec_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list += [dec_slf_attn]\n",
    "\n",
    "        return dec_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "\n",
    "    ids = torch.arange(0, max_len, 1, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSpeech(nn.Module):\n",
    "    \"\"\" FastSpeech \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(FastSpeech, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.variance_adaptor = VarianceAdaptor(model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "\n",
    "        self.mel_linear = nn.Linear(model_config.decoder_dim, mel_config.num_mels)\n",
    "\n",
    "    def mask_tensor(self, mel_output, position, mel_max_length):\n",
    "        lengths = torch.max(position, -1)[0]\n",
    "        mask = ~get_mask_from_lengths(lengths, max_len=mel_max_length)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, mel_output.size(-1))\n",
    "        return mel_output.masked_fill(mask, 0.)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src_seq, \n",
    "        src_pos,\n",
    "        mel_pos=None,\n",
    "        mel_max_length=None,\n",
    "        length_target=None,\n",
    "        energy_target=None,\n",
    "        pitch_target=None,\n",
    "        alpha=1.0\n",
    "    ):\n",
    "        encoder_output, _ = self.encoder(src_seq, src_pos)\n",
    "\n",
    "        (\n",
    "            length_regulator_output,\n",
    "            decoder_pos,\n",
    "            duration_prediction,\n",
    "            energy_prediction,\n",
    "            pitch_prediction,\n",
    "        ) = self.variance_adaptor(\n",
    "            encoder_output,\n",
    "            length_target=length_target,\n",
    "            alpha=alpha,\n",
    "            mel_max_length=mel_max_length,\n",
    "            pitch_target=pitch_target,\n",
    "            energy_target=energy_target,\n",
    "        )\n",
    "\n",
    "        if self.training:\n",
    "            decoder_output = self.decoder(length_regulator_output, mel_pos)\n",
    "        else:\n",
    "            decoder_output = self.decoder(length_regulator_output, decoder_pos)\n",
    "\n",
    "        mel_output = self.mel_linear(decoder_output)\n",
    "\n",
    "        if self.training:\n",
    "            mel_output = self.mask_tensor(mel_output, mel_pos, mel_max_length)\n",
    "        \n",
    "        return (\n",
    "            mel_output,\n",
    "            duration_prediction,\n",
    "            energy_prediction,\n",
    "            pitch_prediction,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        mel, \n",
    "        duration_predicted,\n",
    "        pitch_predicted,\n",
    "        energy_predicted,\n",
    "        mel_target, \n",
    "        duration_target,\n",
    "        pitch_target,\n",
    "        energy_target,\n",
    "    ):\n",
    "        mel_loss = self.mse_loss(mel, mel_target)\n",
    "\n",
    "        duration_predictor_loss = self.l1_loss(duration_predicted, duration_target.float())\n",
    "        energy_predictor_loss = self.l1_loss(energy_predicted, energy_target.float())\n",
    "        pitch_predictor_loss = self.l1_loss(pitch_predicted, pitch_target.float())\n",
    "\n",
    "        return mel_loss, duration_predictor_loss, energy_predictor_loss, pitch_predictor_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler  import OneCycleLR\n",
    "from wandb_writer import WanDBWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastSpeech(model_config)\n",
    "model = model.to(train_config.device)\n",
    "\n",
    "fastspeech_loss = FastSpeechLoss()\n",
    "current_step = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9)\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, **{\n",
    "    \"steps_per_epoch\": len(training_loader) * train_config.batch_expand_size,\n",
    "    \"epochs\": train_config.epochs,\n",
    "    \"anneal_strategy\": \"cos\",\n",
    "    \"max_lr\": train_config.learning_rate,\n",
    "    \"pct_start\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 1/1600000 [06:29<172942:56:12, 389.12s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = WanDBWriter(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 1/1600000 [04:32<121032:45:10, 272.32s/it]\n",
      "  0%|                                                                                                               | 6/1600000 [02:37<12663:11:32, 28.49s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m max_mel_len \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmel_max_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m     24\u001b[0m (\n\u001b[1;32m     25\u001b[0m     mel_output,\n\u001b[1;32m     26\u001b[0m     duration_prediction,\n\u001b[1;32m     27\u001b[0m     energy_prediction,\n\u001b[1;32m     28\u001b[0m     pitch_prediction,\n\u001b[0;32m---> 29\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmel_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_mel_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43menergy_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menergy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpitch_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpitch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calc Loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m losses \u001b[38;5;241m=\u001b[39m fastspeech_loss\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m     41\u001b[0m     mel\u001b[38;5;241m=\u001b[39mmel_output,\n\u001b[1;32m     42\u001b[0m     duration_predicted\u001b[38;5;241m=\u001b[39mduration_prediction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     energy_target\u001b[38;5;241m=\u001b[39menergy,\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mFastSpeech.forward\u001b[0;34m(self, src_seq, src_pos, mel_pos, mel_max_length, length_target, energy_target, pitch_target, alpha)\u001b[0m\n\u001b[1;32m     32\u001b[0m (\n\u001b[1;32m     33\u001b[0m     length_regulator_output,\n\u001b[1;32m     34\u001b[0m     decoder_pos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     energy_target\u001b[38;5;241m=\u001b[39menergy_target,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 48\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength_regulator_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(length_regulator_output, decoder_pos)\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, enc_seq, enc_pos, return_attns)\u001b[0m\n\u001b[1;32m     36\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m enc_seq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_enc(enc_pos)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_stack:\n\u001b[0;32m---> 39\u001b[0m     dec_output, dec_slf_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_pad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_pad_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mslf_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslf_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attns:\n\u001b[1;32m     44\u001b[0m         dec_slf_attn_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [dec_slf_attn]\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mFFTBlock.forward\u001b[0;34m(self, enc_input, non_pad_mask, slf_attn_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_input, non_pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, slf_attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 18\u001b[0m     enc_output, enc_slf_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslf_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslf_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m non_pad_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         enc_output \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m non_pad_mask\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mrepeat(n_head, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (n*b) x .. x ..\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m output, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(n_head, sz_b, len_q, d_v)\n\u001b[1;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(sz_b, len_q, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# b x lq x (n*dv)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mmasked_fill(mask, \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m---> 14\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn, v)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, attn\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1390\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.8/site-packages/torch/nn/functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tqdm_bar = tqdm(total=train_config.epochs * len(training_loader) * train_config.batch_expand_size - current_step)\n",
    "\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "    for i, batchs in enumerate(training_loader):\n",
    "        # real batch start here\n",
    "        for j, db in enumerate(batchs):\n",
    "            current_step += 1\n",
    "            tqdm_bar.update(1)\n",
    "            \n",
    "#             logger.set_step(current_step)\n",
    "\n",
    "            # Get Data\n",
    "            character = db[\"text\"].long().to(train_config.device)\n",
    "            mel_target = db[\"mel_target\"].float().to(train_config.device)\n",
    "            energy = db[\"energy_target\"].int().to(train_config.device)\n",
    "            pitch = db[\"pitch_target\"].int().to(train_config.device)\n",
    "            duration = db[\"duration\"].int().to(train_config.device)\n",
    "            mel_pos = db[\"mel_pos\"].long().to(train_config.device)\n",
    "            src_pos = db[\"src_pos\"].long().to(train_config.device)\n",
    "            max_mel_len = db[\"mel_max_len\"]\n",
    "\n",
    "            # Forward\n",
    "            (\n",
    "                mel_output,\n",
    "                duration_prediction,\n",
    "                energy_prediction,\n",
    "                pitch_prediction,\n",
    "            ) = model(\n",
    "                character,\n",
    "                src_pos,\n",
    "                mel_pos=mel_pos,\n",
    "                mel_max_length=max_mel_len,\n",
    "                length_target=duration,\n",
    "                energy_target=energy,\n",
    "                pitch_target=pitch,\n",
    "            )\n",
    "\n",
    "            # Calc Loss\n",
    "            losses = fastspeech_loss.forward(\n",
    "                mel=mel_output,\n",
    "                duration_predicted=duration_prediction,\n",
    "                pitch_predicted=pitch_prediction,\n",
    "                energy_predicted=energy_prediction,\n",
    "                mel_target=mel_target,\n",
    "                duration_target=duration,\n",
    "                pitch_target=pitch,\n",
    "                energy_target=energy,\n",
    "            )\n",
    "            \n",
    "            total_loss = sum(losses)\n",
    "\n",
    "            # Logger\n",
    "#             t_l = total_loss.detach().cpu().numpy()\n",
    "#             m_l = mel_loss.detach().cpu().numpy()\n",
    "#             d_l = duration_loss.detach().cpu().numpy()\n",
    "\n",
    "#             logger.add_scalar(\"duration_loss\", d_l)\n",
    "#             logger.add_scalar(\"mel_loss\", m_l)\n",
    "#             logger.add_scalar(\"total_loss\", t_l)\n",
    "\n",
    "            # Backward\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Clipping gradients to avoid gradient explosion\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), train_config.grad_clip_thresh)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            if current_step % train_config.save_step == 0:\n",
    "                torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(\n",
    "                )}, os.path.join(train_config.checkpoint_path, 'checkpoint_%d.pth.tar' % current_step))\n",
    "                print(\"save model at step %d ...\" % current_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import waveglow\n",
    "import text\n",
    "import audio\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразовывать мел-спектрограмы в wav будем используя WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaveGlow = utils.get_WaveGlow()\n",
    "WaveGlow = WaveGlow.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('../model_new/checkpoint_225000.pth.tar', map_location='cuda:0')['model'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis(model, text, alpha=1.0):\n",
    "    text = np.array(phn)\n",
    "    text = np.stack([text])\n",
    "    src_pos = np.array([i+1 for i in range(text.shape[1])])\n",
    "    src_pos = np.stack([src_pos])\n",
    "    sequence = torch.from_numpy(text).long().to(train_config.device)\n",
    "    src_pos = torch.from_numpy(src_pos).long().to(train_config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mel = model.forward(sequence, src_pos, alpha=alpha)\n",
    "    return mel[0].cpu().transpose(0, 1), mel.contiguous().transpose(1, 2)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    tests = [ \n",
    "        \"I am very happy to see you again!\",\n",
    "        \"Durian model is a very good speech synthesis!\",\n",
    "        \"When I was twenty, I fell in love with a girl.\",\n",
    "        \"I remove attention module in decoder and use average pooling to implement predicting r frames at once\",\n",
    "        \"You can not improve your past, but you can improve your future. Once time is wasted, life is wasted.\",\n",
    "        \"Death comes to all, but great achievements raise a monument which shall endure until the sun grows old.\"\n",
    "    ]\n",
    "    data_list = list(text.text_to_sequence(test, train_config.text_cleaners) for test in tests)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "data_list = get_data()\n",
    "for speed in [0.8, 1., 1.3]:\n",
    "    for i, phn in tqdm(enumerate(data_list)):\n",
    "        mel, mel_cuda = synthesis(model, phn, speed)\n",
    "        \n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        audio.tools.inv_mel_spec(\n",
    "            mel, f\"results/s={speed}_{i}.wav\"\n",
    "        )\n",
    "        \n",
    "        waveglow.inference.inference(\n",
    "            mel_cuda, WaveGlow,\n",
    "            f\"results/s={speed}_{i}_waveglow.wav\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерим звук с тремя разными скоростями используя возможности фастспича"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FastSpeech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
